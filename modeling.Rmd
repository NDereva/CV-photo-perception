---
title: "R Notebook"
output: html_notebook
---


## Modeling 

Lets try to fit simple model 
```{r message=FALSE}

rm(list = ls())
library(brms)
library(tidybayes)
library(tidyverse)
library(here)
```

```{r}
d <- read_rds(here("raw data", "clean_dat.rds"))
```


Lets start with the linear model just with intercepts for each smile type
Obviously linear model is not perfectly suitable because of the type of response 
variable. For example, the distance between 0 and 1 might be different with the 5 and 6 for.

### Intercept model

```{r}
m.1 <- 
  brm(data = d, 
      family = gaussian,
      cand_competency ~ 0 + cand_smile,   # 0 indicates that there is separate intercept
                                          # for each smile type
      prior = c(prior(normal(0.5, 0.25), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 1,
      file = "fits/m.1")
```

```{r}
print(m.1)
```

```{r}
plot(m.1)
```

```{r}
mcmc_plot(m.1, pars = "b_")
```

The result resemble the one in the blogpost. 
Candidates with small smile look slightly more competent 


Lets try to fit model with random slopes varying across candidates

### Random slopes varying by candidate

```{r}
m.2 <- 
  brm(data = d, 
      family = gaussian,
      cand_competency ~ 0 + cand_smile + # separate intercept for each smile type
        (1 | cand_id),                   # varying intercept by candidates 
      prior = c(prior(normal(0.5, 0.25), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 1,
      file = "fits/m.2")
```
model took 10 seconds to estimate

```{r}
print(m.2)
```


```{r}
mcmc_plot(m.2, pars = "b_")
```



### Varying slopes and intercepts

Lets try to add slopes varying by sex to intercepts

```{r}
m.3 <- 
  brm(data = d, 
      family = gaussian,
      cand_competency ~ 0 + cand_smile + # separate intercept for each smile type
        (1 + cand_sex || cand_id),        # uncorrelated random slope and intercept
      prior = c(prior(normal(0.5, 0.25), class = b),
                prior(exponential(1), class = sigma)),
      iter = 3000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.98),
      seed = 3,
      file = "fits/m.3")
```

```{r}
print(m.3)
```



```{r}
mcmc_plot(m.2, pars = "b_")
```
The estimates are quite the same as in 2nd model 

Just for the sake of and experiment I will try an alternative model specification

```{r}
m.4 <- 
  brm(data = d, 
      family = gaussian,
      cand_competency ~ 0 + cand_smile + cand_sex:resp_sex + (1 | cand_id),
      prior = c(prior(normal(0, 1), class = b),
                prior(exponential(1), class = sigma)),
      control = list(adapt_delta = 0.98),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/m.4")
```

```{r}
print(m.4)
```
 
 This est. errors looks huge. 
 Seems that is because of the multicolinearity
 
```{r}
mcmc_plot(m.4, pars = "cand_sexF")
```


```{r}
mcmc_plot(m.4, pars = "cand_sexM")
```

### Compare models 

```{r}
m.1 <- add_criterion(m.1, criterion = "loo")
m.2 <- add_criterion(m.2, criterion = "loo")
m.3 <- add_criterion(m.3, criterion = "loo")
m.4 <- add_criterion(m.4, criterion = "loo")
```

```{r}
loo_compare(m.1, m.2, m.3, m.4, criterion = "loo") %>% 
  print(simplify = F)
```

As expected, the simplest first model performs much worse 
There is no much of a difference between second and third model 
Fourth performs slightly better then others, but it suffers from multicollinearity


## model with all the data



```{r}
get_prior(data = d, 
      family = gaussian,
      cand_competency ~ 0 + cand_smile + cand_sex + (1 | cand_id) +        
        resp_sex + resp_hire)
```


```{r}
m.5 <- 
  brm(data = d, 
      family = gaussian,
      cand_competency ~ 0 + cand_smile + cand_sex +
         resp_sex + resp_hire + (1 | cand_id),        
      prior = c(prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/m.5")
```

```{r}
print(m.5)
```
This seem much better than 4th model. Standard errors are much narrower.

```{r}
m.5 <- add_criterion(m.5, criterion = "loo")
```

```{r}
loo_compare(m.2, m.4, m.5, criterion = "loo") %>% 
  print(simplify = F)
```



# Cummulative link Model

As I wrote treating ordered categorical outcome as a continuous variable generally 
is not the best idea. 
Lets do it in a proper way



```{r}
m.6 <- 
  brm(data = d, 
      family = cumulative,
      cand_competency_fct ~ 1 + cand_smile,
      prior = c(prior(normal(0, 1.5), class = Intercept),
                prior(normal(0, 0.5), class = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 12,
      file = "fits/m.6")
```

```{r}
print(m.6)
```

```{r}
m.7 <- 
  brm(data = d, 
      family = cumulative,
      cand_competency_fct ~ 1 + cand_smile + cand_sex + (1 | cand_id),
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 0.5), class = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 12,
      file = "fits/m.7")
```

```{r}
print(m.7)
```

```{r}
mcmc_plot(m.7, pars = "b_cand")
```



```{r}
m.8 <- 
  brm(data = d, 
      family = cumulative,
      cand_competency_fct ~ 1 + cand_smile + cand_sex + resp_sex + resp_hire +
        (1 | cand_id),
      prior = c(prior(normal(0, 2), class = Intercept),
                prior(normal(0, 0.5), class = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 12,
      file = "fits/m.8")
```


```{r}
print(m.8)
```

```{r}
mcmc_plot(m.8, pars = "b_cand")
```

```{r}
mcmc_plot(m.8, pars = "b_resp")
```



```{r}
pp_check(m.8, type = "bars", nsamples = 100)
```



Quick model comparison 

```{r}
m.6 <- add_criterion(m.6, criterion = "loo")
m.7 <- add_criterion(m.7, criterion = "loo")
m.8 <- add_criterion(m.8, criterion = "loo")
```

```{r}
loo_compare(m.6, m.7, m.8, criterion = "loo") %>% 
  print(simplify = F)
```

As expected, 8th model performs the best


